{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgZtZJNbiDEEACD0E8vAzw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "First we need to read the articles in the citation format. We stored the document in github for easy access"
      ],
      "metadata": {
        "id": "dBZuychtQHKX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rX18gPiS8p_U"
      },
      "outputs": [],
      "source": [
        "#I loaded the original TXT file\n",
        "import requests\n",
        "# URL of the text file\n",
        "file_url = 'https://raw.githubusercontent.com/jsanc223/dataset/main/africa_research.txt'\n",
        "\n",
        "# I Sent a GET request to the URL and read the response content\n",
        "response = requests.get(file_url)\n",
        "file_contents = response.text\n",
        "\n",
        "# Split the file contents by blank lines to separate the records\n",
        "records = file_contents.split('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the API Key and Institutional Token. Those are stored in config.json local file"
      ],
      "metadata": {
        "id": "8KRHPUvzQb1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('config.json') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "api_key = config['api_key']\n",
        "inst_token = config['inst_token']"
      ],
      "metadata": {
        "id": "ne5QZtAsU2ZC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterate to each journal article and get only the title, DOI, Elsevier URL and Article year. Then, store it into a dataframe to manage data properly"
      ],
      "metadata": {
        "id": "hc5a4IeHQ2nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initializing empty lists to store the parsed values\n",
        "ti_values = []\n",
        "do_values = []\n",
        "ur_values = []\n",
        "py_values = []\n",
        "\n",
        "# Iterate over each record\n",
        "for record in records:\n",
        "    lines = record.split('\\n')\n",
        "    ti_value = None\n",
        "    do_value = None\n",
        "    ur_value = None\n",
        "    py_value = None\n",
        "\n",
        "    # Extracting the values for 'TI', 'DO', 'UR', and 'PY' fields\n",
        "    for line in lines:\n",
        "        if line.startswith('TI  - '): # Title\n",
        "            ti_value = line[6:].strip()\n",
        "        elif line.startswith('DO  - '): # DOI\n",
        "            do_value = line[6:].strip()\n",
        "        elif line.startswith('UR  - '):  # URLs\n",
        "            ur_value = line[6:].strip()\n",
        "        elif line.startswith('PY  - '):  # Year\n",
        "            py_value = line[6:].strip()\n",
        "\n",
        "    # Appending the values to the respective lists\n",
        "    if ti_value is not None or do_value is not None or ur_value is not None or py_value is not None:\n",
        "        ti_values.append(ti_value)\n",
        "        do_values.append(do_value)\n",
        "        ur_values.append(ur_value)\n",
        "        py_values.append(py_value)\n",
        "\n",
        "# I created a DataFrame from the parsed values\n",
        "df = pd.DataFrame({'TI': ti_values, 'DO': do_values, 'UR': ur_values, 'PY': py_values})  # Add 'PY' to the DataFrame\n",
        "df\n"
      ],
      "metadata": {
        "id": "57rxYHyG8uo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing purposes, I grab a sample of the first ten records from the actual dataframe."
      ],
      "metadata": {
        "id": "PNp8pNZZmcIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(10)"
      ],
      "metadata": {
        "id": "9FFwYriwfj7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nSITZTTRnJRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# I created a copy of the original DataFrame before making modifications\n",
        "df_copy = df.copy()\n",
        "\n",
        "# I replaced the empty strings and other potential \"null\" values with NaN\n",
        "df_copy.replace({\"\": np.nan, \" \": np.nan, None: np.nan}, inplace=True)\n",
        "\n",
        "# Then I applied the same filtering as before:\n",
        "df_null = df_copy[df_copy['DO'].isnull()]\n",
        "df_not_null = df_copy[df_copy['DO'].notnull()]\n"
      ],
      "metadata": {
        "id": "ReiozRBy59MS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate a new dataframe withou duplicates DOI from the initial dataframe. In addition, the df_subset dataframe will be exported into a csv format for the visualization tool. Also, we generate a new dataframe, Author_df, where will initiate the columns ID (unique identifier) and Study_ID which is the ID from the df_subset datrame and needed for the visualization tool for references purposes. Also, I initialized the other columns to be filled out later."
      ],
      "metadata": {
        "id": "n529fUM2S5Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I removed the duplicates based on the 'DO' column\n",
        "df_not_null_no_duplicates = df_not_null.drop_duplicates('DO', keep='first')\n",
        "df_not_null_no_duplicates\n",
        "df_subset = df_not_null_no_duplicates.copy()\n",
        "df_subset.reset_index(inplace=True)\n",
        "df_subset.rename(columns={'index': 'ID'}, inplace=True)\n",
        "\n",
        "Author_df = pd.DataFrame()\n",
        "\n",
        "# I added the columns to Author_df\n",
        "Author_df = pd.DataFrame()\n",
        "Author_df['ID'] = df_subset.index\n",
        "Author_df['Study_ID'] = df_subset['ID']\n",
        "Author_df['author'] = np.nan\n",
        "Author_df['affiliation_id'] = np.nan\n",
        "Author_df['affiliation_info'] = np.nan\n",
        "Author_df['affiliation_city'] = np.nan\n",
        "Author_df['affiliation_country'] = np.nan\n",
        "\n",
        "df_subset"
      ],
      "metadata": {
        "id": "fC88xJpP_wmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export to csv df_subset which contains only articles information such as, ID (Unique identifier), TI (Title), DO(DOI), UR (Elsevier URL) and PY (Article year)"
      ],
      "metadata": {
        "id": "-szbccEBSJPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export author_df to a CSV file\n",
        "df_subset.to_csv('Articles.csv', index=False)"
      ],
      "metadata": {
        "id": "0mcBhTKrR9lK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code is the critical part of this process. It has two functions to retrieve and organize author and affiliation information from the Elsevier API using article DOI values.\n",
        "\n",
        "1. get_author_info(DO_value):\n",
        "   - Inputs: DO_value (a DOI value).\n",
        "   - Functionality: Fetches the abstract of the article with the given DOI from the Elsevier API and extracts authors and their affiliations. Sends additional requests to obtain detailed affiliation info.\n",
        "   - Outputs: Lists of authors' names, affiliation IDs, names, cities, and countries.\n",
        "\n",
        "2. populate_author_df(df_subset):\n",
        "   - Inputs: df_subset (a DataFrame subset).\n",
        "   - Functionality: Iterates over the DataFrame rows, calls get_author_info() to obtain author and affiliation info for each DOI, creates a mini DataFrame for each DOI, and appends it to a list.\n",
        "   - Outputs: A concatenated DataFrame, Author_df, of all the mini DataFrames.\n"
      ],
      "metadata": {
        "id": "0lVYNwkXmRDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time  # import the time module\n",
        "\n",
        "def get_author_info(DO_value):\n",
        "    headers = {\n",
        "        \"X-ELS-APIKey\": api_key,\n",
        "        \"X-ELS-Insttoken\": inst_token,\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    url = f\"https://api.elsevier.com/content/abstract/doi/{DO_value}\"\n",
        "    time.sleep(1)\n",
        "    response = requests.get(url, headers=headers)\n",
        "    data = response.json()\n",
        "\n",
        "    authors = []\n",
        "    affiliation_ids = []\n",
        "    affiliation_info_list = []\n",
        "    affiliation_city_list = []\n",
        "    affiliation_country_list = []\n",
        "\n",
        "    def get_affiliation_info(affiliation_id):\n",
        "        url = f\"https://api.elsevier.com/content/search/affiliation?query=af-id({affiliation_id})\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        entry = data['search-results']['entry'][0]\n",
        "        affiliation_info = entry['affiliation-name'] if 'affiliation-name' in entry else None\n",
        "        affiliation_city = entry['city'] if 'city' in entry else None\n",
        "        affiliation_country = entry['country'] if 'country' in entry else None\n",
        "\n",
        "        return affiliation_info, affiliation_city, affiliation_country\n",
        "\n",
        "    try:\n",
        "        author_data = data['abstracts-retrieval-response']['authors']['author']\n",
        "        if isinstance(author_data, list):\n",
        "            for author in author_data:\n",
        "                authors.append(author['preferred-name']['ce:indexed-name'])\n",
        "                affiliations = author.get('affiliation', [])\n",
        "                if isinstance(affiliations, list):\n",
        "                    for aff in affiliations:\n",
        "                        affiliation_ids.append(aff.get('@id', None) if isinstance(aff, dict) else None)\n",
        "                else:\n",
        "                    affiliation_ids.append(affiliations.get('@id', None) if isinstance(affiliations, dict) else None)\n",
        "        else:\n",
        "            authors.append(author_data['preferred-name']['ce:indexed-name'])\n",
        "            affiliations = author_data.get('affiliation', [])\n",
        "            if isinstance(affiliations, list):\n",
        "                for aff in affiliations:\n",
        "                    affiliation_ids.append(aff.get('@id', None) if isinstance(aff, dict) else None)\n",
        "            else:\n",
        "                affiliation_ids.append(affiliations.get('@id', None) if isinstance(affiliations, dict) else None)\n",
        "\n",
        "        for affiliation_id in affiliation_ids:\n",
        "            if affiliation_id:\n",
        "                info, city, country = get_affiliation_info(affiliation_id)\n",
        "                affiliation_info_list.append(info or 'N/A')\n",
        "                affiliation_city_list.append(city or 'N/A')\n",
        "                affiliation_country_list.append(country or 'N/A')\n",
        "            else:\n",
        "                affiliation_info_list.append('N/A')\n",
        "                affiliation_city_list.append('N/A')\n",
        "                affiliation_country_list.append('N/A')\n",
        "\n",
        "        return authors, affiliation_ids, affiliation_info_list, affiliation_city_list, affiliation_country_list\n",
        "\n",
        "    except KeyError:\n",
        "        return [], [], [], [], []\n",
        "\n",
        "def populate_author_df(df_subset):\n",
        "    frames = []\n",
        "    for index, row in df_subset.iterrows():\n",
        "        doi = row['DO']\n",
        "        id = row['ID']  # 'ID' column from df_subset\n",
        "\n",
        "        try:\n",
        "            authors, affiliation_ids, affiliation_info_list, affiliation_city_list, affiliation_country_list = get_author_info(doi)\n",
        "\n",
        "            # Check if authors were found\n",
        "            if not authors:\n",
        "                print(f\"No authors found for ID: {id}, DO: {doi}\")\n",
        "                continue\n",
        "\n",
        "            mini_df = pd.DataFrame({\n",
        "                'ID': [id] * len(authors),  # Use 'ID' instead of 'Study_ID'\n",
        "                'Study_ID': [id] * len(authors),  # Populate with the same 'ID' if 'Study_ID' is not a separate column\n",
        "                'author': authors,\n",
        "                'affiliation_id': affiliation_ids,\n",
        "                'affiliation_info': affiliation_info_list,\n",
        "                'affiliation_city': affiliation_city_list,\n",
        "                'affiliation_country': affiliation_country_list\n",
        "            })\n",
        "            frames.append(mini_df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing DOI: {doi}. Error: {str(e)}\")\n",
        "\n",
        "    if frames:\n",
        "        Author_df = pd.concat(frames, ignore_index=True)\n",
        "    else:\n",
        "        print(\"No authors found in the provided data subset.\")\n",
        "        Author_df = pd.DataFrame()  # return an empty DataFrame\n",
        "\n",
        "    return Author_df"
      ],
      "metadata": {
        "id": "GpK-FTG-HcYW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export to csv Author_df which contains the author information such as, ID (unique identifier), Study_ID (ID from the articles), Author Full Name, Affiliation ID, Affiliation Information, Affiliation City and Affiliation Country."
      ],
      "metadata": {
        "id": "S4Dk81NEkI_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Author_df = populate_author_df(df_subset)\n",
        "Author_df.to_csv('Author.csv', index=False)\n",
        "#Author_df"
      ],
      "metadata": {
        "id": "r24zDbEOgCfG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}